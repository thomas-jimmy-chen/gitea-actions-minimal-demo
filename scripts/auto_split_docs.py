#!/usr/bin/env python3
"""
è‡ªå‹•æ–‡æª”åˆ†å‰²å·¥å…· - Auto Document Splitter
ç”¨æ–¼è‡ªå‹•åµæ¸¬ä¸¦åˆ†å‰²å¤§å‹ Markdown æ–‡æª”ç‚º AI å‹å–„çš„å¯è®€å–å¤§å°

åŠŸèƒ½ï¼š
1. åµæ¸¬æ–‡ä»¶å¤§å°ä¸¦ä¼°ç®— token æ•¸é‡
2. è‡ªå‹•è­˜åˆ¥ç« ç¯€é‚Šç•Œï¼ˆ## æˆ–æ›´é«˜éšæ¨™é¡Œï¼‰
3. æ™ºèƒ½é¸æ“‡æœ€ä½³åˆ†å‰²é»
4. ç”Ÿæˆåˆ†å‰²æ–‡ä»¶å’Œå°èˆªç´¢å¼•
5. è¼¸å‡ºé©—è­‰å ±å‘Š

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python scripts/auto_split_docs.py docs/CLAUDE_CODE_HANDOVER-2.md
    python scripts/auto_split_docs.py docs/CLAUDE_CODE_HANDOVER-2.md --max-tokens 20000
    python scripts/auto_split_docs.py docs/CLAUDE_CODE_HANDOVER-2.md --dry-run
"""

import re
import argparse
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import json


@dataclass
class Chapter:
    """ç« ç¯€è³‡è¨Š"""
    level: int  # æ¨™é¡Œå±¤ç´š (1-6)
    title: str  # ç« ç¯€æ¨™é¡Œ
    line_start: int  # èµ·å§‹è¡Œè™Ÿ
    line_end: int  # çµæŸè¡Œè™Ÿ
    content: List[str]  # å…§å®¹è¡Œ
    estimated_tokens: int  # ä¼°ç®— token æ•¸


@dataclass
class SplitPoint:
    """åˆ†å‰²é»è³‡è¨Š"""
    line_number: int  # åˆ†å‰²è¡Œè™Ÿ
    chapter_title: str  # è©²é»çš„ç« ç¯€æ¨™é¡Œ
    before_tokens: int  # ä¹‹å‰çš„ç´¯è¨ˆ token
    after_tokens: int  # ä¹‹å¾Œçš„ç´¯è¨ˆ token
    score: float  # åˆ†å‰²é»å“è³ªåˆ†æ•¸ï¼ˆ0-100ï¼‰


@dataclass
class SplitSegment:
    """åˆ†å‰²æ®µè½è³‡è¨Š"""
    segment_id: str  # æ®µè½ ID (å¦‚ "2A", "2B")
    line_start: int  # èµ·å§‹è¡Œè™Ÿ
    line_end: int  # çµæŸè¡Œè™Ÿ
    estimated_tokens: int  # ä¼°ç®— token æ•¸
    chapters: List[str]  # åŒ…å«çš„ç« ç¯€æ¨™é¡Œ
    output_file: str  # è¼¸å‡ºæª”æ¡ˆåç¨±


class TokenEstimator:
    """Token æ•¸é‡ä¼°ç®—å™¨"""

    # Token ä¼°ç®—ä¿‚æ•¸
    CHINESE_CHAR_RATIO = 2.5  # ä¸­æ–‡å­—å…ƒç´„ 2.5 tokens/å­—
    ENGLISH_WORD_RATIO = 1.3  # è‹±æ–‡å–®å­—ç´„ 1.3 tokens/å­—
    CODE_CHAR_RATIO = 1.5  # ç¨‹å¼ç¢¼ç´„ 1.5 tokens/å­—

    @staticmethod
    def estimate_line_tokens(line: str) -> int:
        """ä¼°ç®—å–®è¡Œçš„ token æ•¸é‡"""
        # ç¨‹å¼ç¢¼å€å¡Šï¼ˆ``` åŒ…åœï¼‰
        if line.strip().startswith('```'):
            return 2

        # çµ±è¨ˆå­—å…ƒé¡å‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', line))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', line))
        code_chars = len(re.findall(r'[{}()\[\]<>.,;:\'"`]', line))

        # è¨ˆç®— tokens
        tokens = 0
        tokens += chinese_chars * TokenEstimator.CHINESE_CHAR_RATIO
        tokens += english_words * TokenEstimator.ENGLISH_WORD_RATIO
        tokens += code_chars * TokenEstimator.CODE_CHAR_RATIO

        # åŸºç¤ tokenï¼ˆæ¯è¡Œè‡³å°‘ 1 tokenï¼‰
        return max(1, int(tokens))

    @staticmethod
    def estimate_tokens(lines: List[str]) -> int:
        """ä¼°ç®—å¤šè¡Œçš„ç¸½ token æ•¸é‡"""
        return sum(TokenEstimator.estimate_line_tokens(line) for line in lines)


class ChapterParser:
    """ç« ç¯€è§£æå™¨"""

    @staticmethod
    def parse_chapters(lines: List[str]) -> List[Chapter]:
        """è§£ææ–‡æª”ä¸­çš„æ‰€æœ‰ç« ç¯€"""
        chapters = []
        current_chapter = None

        for i, line in enumerate(lines):
            # æª¢æ¸¬æ¨™é¡Œï¼ˆ# é–‹é ­ï¼‰
            heading_match = re.match(r'^(#{1,6})\s+(.+)$', line)

            if heading_match:
                # çµæŸä¸Šä¸€å€‹ç« ç¯€
                if current_chapter:
                    current_chapter.line_end = i - 1
                    current_chapter.estimated_tokens = TokenEstimator.estimate_tokens(
                        current_chapter.content
                    )
                    chapters.append(current_chapter)

                # é–‹å§‹æ–°ç« ç¯€
                level = len(heading_match.group(1))
                title = heading_match.group(2).strip()
                current_chapter = Chapter(
                    level=level,
                    title=title,
                    line_start=i,
                    line_end=i,
                    content=[line],
                    estimated_tokens=0
                )
            elif current_chapter:
                current_chapter.content.append(line)

        # è™•ç†æœ€å¾Œä¸€å€‹ç« ç¯€
        if current_chapter:
            current_chapter.line_end = len(lines) - 1
            current_chapter.estimated_tokens = TokenEstimator.estimate_tokens(
                current_chapter.content
            )
            chapters.append(current_chapter)

        return chapters


class DocumentSplitter:
    """æ–‡æª”åˆ†å‰²å™¨"""

    def __init__(self, max_tokens: int = 25000, min_tokens: int = 15000):
        self.max_tokens = max_tokens
        self.min_tokens = min_tokens

    def find_split_points(
        self,
        chapters: List[Chapter],
        total_tokens: int
    ) -> List[SplitPoint]:
        """æ‰¾å‡ºæœ€ä½³åˆ†å‰²é»"""

        # å¦‚æœä¸éœ€è¦åˆ†å‰²
        if total_tokens <= self.max_tokens:
            return []

        # è¨ˆç®—éœ€è¦åˆ†æˆå¹¾æ®µ
        num_segments = (total_tokens // self.max_tokens) + 1
        ideal_tokens_per_segment = total_tokens / num_segments

        split_points = []
        cumulative_tokens = 0
        last_split = 0

        for i, chapter in enumerate(chapters):
            cumulative_tokens += chapter.estimated_tokens

            # åªè€ƒæ…®äºŒç´šæ¨™é¡Œï¼ˆ##ï¼‰ä½œç‚ºåˆ†å‰²é»
            if chapter.level != 2:
                continue

            # è¨ˆç®—åˆ†å‰²å“è³ªåˆ†æ•¸
            tokens_before = cumulative_tokens
            tokens_after = total_tokens - cumulative_tokens

            # åˆ†æ•¸è¨ˆç®—ï¼š
            # 1. èˆ‡ç†æƒ³å¤§å°çš„æ¥è¿‘ç¨‹åº¦ï¼ˆ50%ï¼‰
            # 2. é¿å…éå°æˆ–éå¤§çš„æ®µè½ï¼ˆ30%ï¼‰
            # 3. æ®µè½å¤§å°å¹³è¡¡åº¦ï¼ˆ20%ï¼‰

            ideal_diff = abs(tokens_before - ideal_tokens_per_segment * (len(split_points) + 1))
            ideal_score = max(0, 100 - (ideal_diff / ideal_tokens_per_segment * 100))

            size_score = 100
            if tokens_before < self.min_tokens or tokens_after < self.min_tokens:
                size_score = 0

            balance_score = 100 - abs(tokens_before - tokens_after) / total_tokens * 100

            total_score = ideal_score * 0.5 + size_score * 0.3 + balance_score * 0.2

            split_point = SplitPoint(
                line_number=chapter.line_start,
                chapter_title=chapter.title,
                before_tokens=tokens_before,
                after_tokens=tokens_after,
                score=total_score
            )

            split_points.append(split_point)

        # é¸æ“‡æœ€ä½³åˆ†å‰²é»
        if not split_points:
            return []

        # ä¾åˆ†æ•¸æ’åºä¸¦é¸æ“‡æœ€ä½³çš„å¹¾å€‹
        split_points.sort(key=lambda sp: sp.score, reverse=True)

        # é¸æ“‡éœ€è¦çš„åˆ†å‰²é»æ•¸é‡
        num_splits = num_segments - 1
        best_splits = split_points[:num_splits]

        # ä¾è¡Œè™Ÿæ’åº
        best_splits.sort(key=lambda sp: sp.line_number)

        return best_splits

    def create_segments(
        self,
        lines: List[str],
        chapters: List[Chapter],
        split_points: List[SplitPoint],
        base_filename: str
    ) -> List[SplitSegment]:
        """å‰µå»ºåˆ†å‰²æ®µè½"""

        segments = []
        segment_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']

        # æå–åŸºç¤æª”åå’Œç·¨è™Ÿï¼ˆå¦‚ CLAUDE_CODE_HANDOVER-2.md -> 2ï¼‰
        base_match = re.match(r'(.+)-(\d+)(\.md)?$', base_filename)
        if base_match:
            base_name = base_match.group(1)
            base_number = base_match.group(2)
        else:
            base_name = Path(base_filename).stem
            base_number = ""

        # å®šç¾©æ®µè½é‚Šç•Œ
        boundaries = [0] + [sp.line_number for sp in split_points] + [len(lines)]

        for i in range(len(boundaries) - 1):
            start = boundaries[i]
            end = boundaries[i + 1]

            segment_label = f"{base_number}{segment_labels[i]}" if base_number else segment_labels[i]
            segment_lines = lines[start:end]

            # æ‰¾å‡ºæ­¤æ®µè½åŒ…å«çš„ç« ç¯€
            segment_chapters = [
                ch.title for ch in chapters
                if ch.line_start >= start and ch.line_end < end and ch.level == 2
            ]

            segment = SplitSegment(
                segment_id=segment_label,
                line_start=start + 1,  # è½‰ç‚º 1-based
                line_end=end,
                estimated_tokens=TokenEstimator.estimate_tokens(segment_lines),
                chapters=segment_chapters,
                output_file=f"{base_name}-{segment_label}.md"
            )

            segments.append(segment)

        return segments


class IndexGenerator:
    """ç´¢å¼•ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_index(
        segments: List[SplitSegment],
        original_file: str,
        total_lines: int,
        total_tokens: int
    ) -> str:
        """ç”Ÿæˆå°èˆªç´¢å¼•å…§å®¹"""

        base_name = Path(original_file).stem

        index_content = f"""# {base_name} - ç´¢å¼•

> **ğŸ“‘ å°èˆªç´¢å¼•** - æœ¬æ–‡æª”å·²æ‹†åˆ†ç‚ºå¤šå€‹å­æ®µï¼Œä»¥ä¾¿ AI åŠ©æ‰‹é †åˆ©è®€å–

---

## ğŸ“š æ–‡æª”çµæ§‹

æœ¬æ®µåŸæ–‡æª”å› ç¯‡å¹…éå¤§ï¼ˆ{total_lines:,} è¡Œï¼Œ{total_tokens:,} tokensï¼‰ï¼Œå·²æ‹†åˆ†ç‚ºä»¥ä¸‹ {len(segments)} å€‹å­æ®µï¼š

"""

        # ç‚ºæ¯å€‹æ®µè½ç”Ÿæˆèªªæ˜
        for i, segment in enumerate(segments):
            index_content += f"""### ğŸ“„ ç¬¬ {segment.segment_id} æ®µ

**æª”æ¡ˆ**: [{segment.output_file}](./{segment.output_file})

**è¡Œæ•¸**: {segment.line_start}-{segment.line_end} ({segment.line_end - segment.line_start + 1:,} è¡Œ)

**ä¼°ç®— Tokens**: ~{segment.estimated_tokens:,}

**åŒ…å«ç« ç¯€**:
"""
            for chapter in segment.chapters[:5]:  # åªåˆ—å‡ºå‰ 5 å€‹ç« ç¯€
                index_content += f"- {chapter}\n"

            if len(segment.chapters) > 5:
                index_content += f"- (...é‚„æœ‰ {len(segment.chapters) - 5} å€‹ç« ç¯€)\n"

            index_content += "\n"

        index_content += f"""---

## ğŸ¯ é–±è®€å»ºè­°

### å°æ–¼æ–°æ¥æ‰‹çš„ AI åŠ©æ‰‹

**é †åºé–±è®€**:
"""
        for i, segment in enumerate(segments, 1):
            index_content += f"{i}. [{segment.output_file}](./{segment.output_file})\n"

        index_content += f"""
---

## ğŸ” æ–‡æª”çµ±è¨ˆ

| æ®µè½ | è¡Œæ•¸ | Token ä¼°ç®— | ç‹€æ…‹ |
|------|------|-----------|------|
"""

        for segment in segments:
            line_count = segment.line_end - segment.line_start + 1
            index_content += f"| {segment.segment_id} æ®µ | {line_count:,} è¡Œ | ~{segment.estimated_tokens:,} | âœ… |\n"

        total_line_count = sum(seg.line_end - seg.line_start + 1 for seg in segments)
        index_content += f"| **ç¸½è¨ˆ** | **{total_line_count:,} è¡Œ** | **~{total_tokens:,}** | âœ… |\n"

        index_content += f"""
---

## ğŸ“Œ é‡è¦æé†’

**è‡ªå‹•åˆ†å‰²è³‡è¨Š**:
- åŸæ–‡æª”ï¼š{total_lines:,} è¡Œï¼Œ~{total_tokens:,} tokens
- Token é™åˆ¶ï¼š25,000 tokens
- åˆ†å‰²æ®µæ•¸ï¼š{len(segments)} æ®µ
- ç”Ÿæˆæ–¹å¼ï¼šè‡ªå‹•åŒ–å·¥å…· (auto_split_docs.py)

---

*ç´¢å¼•æ–‡æª” | è‡ªå‹•ç”Ÿæˆæ™‚é–“: {{timestamp}} | å·¥å…·: auto_split_docs.py*
"""

        return index_content

    @staticmethod
    def add_navigation_header(
        segment: SplitSegment,
        segments: List[SplitSegment],
        content: str,
        base_name: str
    ) -> str:
        """ç‚ºæ®µè½æ·»åŠ å°èˆªæ¨™é ­"""

        current_idx = segments.index(segment)

        # ç”Ÿæˆå°èˆªè³‡è¨Š
        nav_header = f"""# {base_name} (ç¬¬ {segment.segment_id} æ®µ)

> **åˆ†æ®µè³‡è¨Š**: æœ¬æ–‡æª”å…± {len(segments)} æ®µ
> - ğŸ“„ **ç•¶å‰**: ç¬¬ {segment.segment_id} æ®µ
"""

        # ä¸‹ä¸€æ®µé€£çµ
        if current_idx < len(segments) - 1:
            next_seg = segments[current_idx + 1]
            nav_header += f"> - â¡ï¸ **ä¸‹ä¸€æ®µ**: [{next_seg.output_file}](./{next_seg.output_file})\n"

        # ä¸Šä¸€æ®µé€£çµ
        if current_idx > 0:
            prev_seg = segments[current_idx - 1]
            nav_header += f"> - â¬…ï¸ **ä¸Šä¸€æ®µ**: [{prev_seg.output_file}](./{prev_seg.output_file})\n"

        # ç´¢å¼•é€£çµ
        nav_header += f"> - ğŸ“‘ **å®Œæ•´ç´¢å¼•**: [è¿”å›ç´¢å¼•](./{base_name}.md)\n"
        nav_header += "\n---\n\n"

        # æ®µè½çµå°¾
        footer = f"\n\n---\n\n**æœ¬æ®µçµæŸ**\n\n"

        if current_idx < len(segments) - 1:
            next_seg = segments[current_idx + 1]
            footer += f"ğŸ“ **ç¹¼çºŒé–±è®€**: [{next_seg.output_file}](./{next_seg.output_file})\n"

        footer += "\n---\n"

        return nav_header + content + footer


def safe_print(text: str = "") -> None:
    """å®‰å…¨çš„ print å‡½æ•¸ï¼Œè™•ç† Windows ç·¨ç¢¼å•é¡Œ"""
    try:
        print(text)
    except UnicodeEncodeError:
        # Windows CMD ä¸æ”¯æ´æŸäº›å­—å…ƒï¼Œç§»é™¤ emoji å¾Œé‡è©¦
        import re
        # ç§»é™¤ emoji å’Œç‰¹æ®Šç¬¦è™Ÿ
        clean_text = re.sub(r'[^\x00-\x7F\u4e00-\u9fff]+', '', text)
        print(clean_text)


def analyze_document(file_path: Path) -> Dict:
    """åˆ†ææ–‡æª”ä¸¦è¿”å›è©³ç´°è³‡è¨Š"""

    safe_print(f"ğŸ“– æ­£åœ¨åˆ†ææ–‡æª”: {file_path.name}")
    safe_print("=" * 60)

    # è®€å–æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # åŸºæœ¬è³‡è¨Š
    total_lines = len(lines)
    total_chars = sum(len(line) for line in lines)
    file_size_kb = file_path.stat().st_size / 1024

    safe_print(f"ğŸ“„ åŸºæœ¬è³‡è¨Š:")
    safe_print(f"   - ç¸½è¡Œæ•¸: {total_lines:,} è¡Œ")
    safe_print(f"   - ç¸½å­—å…ƒæ•¸: {total_chars:,} å­—å…ƒ")
    safe_print(f"   - æª”æ¡ˆå¤§å°: {file_size_kb:.2f} KB")

    # Token ä¼°ç®—
    total_tokens = TokenEstimator.estimate_tokens(lines)
    safe_print(f"   - ä¼°ç®— Tokens: ~{total_tokens:,} tokens")

    # ç« ç¯€è§£æ
    chapters = ChapterParser.parse_chapters(lines)
    level_counts = {}
    for ch in chapters:
        level_counts[ch.level] = level_counts.get(ch.level, 0) + 1

    safe_print(f"\nğŸ“š ç« ç¯€çµæ§‹:")
    for level in sorted(level_counts.keys()):
        safe_print(f"   - Level {level} ({'#' * level}): {level_counts[level]} å€‹ç« ç¯€")

    # åˆ¤æ–·æ˜¯å¦éœ€è¦åˆ†å‰²
    needs_split = total_tokens > 25000

    safe_print(f"\nğŸ” åˆ†å‰²éœ€æ±‚:")
    if needs_split:
        safe_print(f"   âš ï¸  éœ€è¦åˆ†å‰² (è¶…é 25,000 token é™åˆ¶)")
        recommended_segments = (total_tokens // 20000) + 1
        safe_print(f"   ğŸ“Š å»ºè­°åˆ†å‰²ç‚º: {recommended_segments} æ®µ")
    else:
        safe_print(f"   âœ… ç„¡éœ€åˆ†å‰² (ä½æ–¼ 25,000 token é™åˆ¶)")

    return {
        'lines': lines,
        'total_lines': total_lines,
        'total_tokens': total_tokens,
        'chapters': chapters,
        'needs_split': needs_split,
        'file_size_kb': file_size_kb
    }


def perform_split(
    file_path: Path,
    analysis: Dict,
    max_tokens: int = 25000,
    dry_run: bool = False
) -> None:
    """åŸ·è¡Œæ–‡æª”åˆ†å‰²"""

    lines = analysis['lines']
    chapters = analysis['chapters']
    total_tokens = analysis['total_tokens']

    safe_print("\n" + "=" * 60)
    safe_print("ğŸ”ª é–‹å§‹åˆ†å‰²ç¨‹åº")
    safe_print("=" * 60)

    # å‰µå»ºåˆ†å‰²å™¨
    splitter = DocumentSplitter(max_tokens=max_tokens)

    # å°‹æ‰¾åˆ†å‰²é»
    safe_print("\nğŸ¯ å°‹æ‰¾æœ€ä½³åˆ†å‰²é»...")
    split_points = splitter.find_split_points(chapters, total_tokens)

    if not split_points:
        safe_print("   â„¹ï¸  ç„¡éœ€åˆ†å‰²æˆ–ç„¡æ³•æ‰¾åˆ°åˆé©çš„åˆ†å‰²é»")
        return

    safe_print(f"   âœ… æ‰¾åˆ° {len(split_points)} å€‹åˆ†å‰²é»:\n")
    for i, sp in enumerate(split_points, 1):
        safe_print(f"   {i}. è¡Œ {sp.line_number}: {sp.chapter_title}")
        safe_print(f"      - ä¹‹å‰: ~{sp.before_tokens:,} tokens")
        safe_print(f"      - ä¹‹å¾Œ: ~{sp.after_tokens:,} tokens")
        safe_print(f"      - åˆ†æ•¸: {sp.score:.1f}/100")
        safe_print()

    # å‰µå»ºæ®µè½
    safe_print("ğŸ“ å‰µå»ºåˆ†å‰²æ®µè½...")
    base_filename = file_path.stem
    segments = splitter.create_segments(lines, chapters, split_points, base_filename)

    safe_print(f"   âœ… å‰µå»º {len(segments)} å€‹æ®µè½:\n")
    for seg in segments:
        safe_print(f"   - {seg.segment_id} æ®µ: {seg.output_file}")
        safe_print(f"     è¡Œæ•¸: {seg.line_start}-{seg.line_end} ({seg.line_end - seg.line_start + 1:,} è¡Œ)")
        safe_print(f"     Tokens: ~{seg.estimated_tokens:,}")
        safe_print(f"     ç« ç¯€æ•¸: {len(seg.chapters)}")
        safe_print()

    if dry_run:
        safe_print("ğŸ” Dry-run æ¨¡å¼ - ä¸å¯«å…¥æª”æ¡ˆ")
        return

    # ç”Ÿæˆæ–‡ä»¶
    safe_print("ğŸ’¾ å¯«å…¥åˆ†å‰²æ–‡ä»¶...")
    output_dir = file_path.parent

    for i, segment in enumerate(segments):
        output_path = output_dir / segment.output_file

        # æå–æ®µè½å…§å®¹
        segment_lines = lines[segment.line_start - 1:segment.line_end]
        segment_content = ''.join(segment_lines)

        # æ·»åŠ å°èˆªæ¨™é ­
        content_with_nav = IndexGenerator.add_navigation_header(
            segment, segments, segment_content, base_filename
        )

        # å¯«å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content_with_nav)

        safe_print(f"   âœ… {output_path.name} ({len(segment_lines):,} è¡Œ)")

    # ç”Ÿæˆç´¢å¼•
    safe_print("\nğŸ“‘ ç”Ÿæˆå°èˆªç´¢å¼•...")
    index_content = IndexGenerator.generate_index(
        segments, base_filename, analysis['total_lines'], total_tokens
    )

    # æ›¿æ›æ™‚é–“æˆ³
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    index_content = index_content.replace('{timestamp}', timestamp)

    # å¯«å…¥ç´¢å¼•ï¼ˆè¦†è“‹åŸæ–‡ä»¶ï¼‰
    index_path = output_dir / f"{base_filename}.md"
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(index_content)

    safe_print(f"   âœ… {index_path.name} (ç´¢å¼•æ–‡ä»¶)")

    safe_print("\n" + "=" * 60)
    safe_print("âœ… åˆ†å‰²å®Œæˆï¼")
    safe_print("=" * 60)

    # ç”Ÿæˆé©—è­‰å ±å‘Š
    safe_print("\nğŸ“‹ é©—è­‰å ±å‘Š:")
    safe_print(f"   - åŸå§‹æ–‡ä»¶: {analysis['total_lines']:,} è¡Œ, ~{total_tokens:,} tokens")
    safe_print(f"   - åˆ†å‰²æ®µæ•¸: {len(segments)} æ®µ")
    safe_print(f"   - è¼¸å‡ºæ–‡ä»¶: {len(segments) + 1} å€‹ ({len(segments)} æ®µ + 1 ç´¢å¼•)")
    safe_print(f"\n   è«‹ä½¿ç”¨ AI åŠ©æ‰‹é©—è­‰ä»¥ä¸‹æ–‡ä»¶å¯æ­£å¸¸è®€å–:")
    for seg in segments:
        safe_print(f"   - {seg.output_file}")
    safe_print(f"   - {base_filename}.md (ç´¢å¼•)")


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description='è‡ªå‹•æ–‡æª”åˆ†å‰²å·¥å…· - å°‡å¤§å‹ Markdown æ–‡æª”åˆ†å‰²ç‚º AI å‹å–„çš„å¤§å°',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  %(prog)s docs/CLAUDE_CODE_HANDOVER-2.md
  %(prog)s docs/CLAUDE_CODE_HANDOVER-2.md --max-tokens 20000
  %(prog)s docs/CLAUDE_CODE_HANDOVER-2.md --dry-run
  %(prog)s docs/CLAUDE_CODE_HANDOVER-2.md --analyze-only
        """
    )

    parser.add_argument(
        'file',
        type=str,
        help='è¦åˆ†å‰²çš„ Markdown æ–‡ä»¶è·¯å¾‘'
    )

    parser.add_argument(
        '--max-tokens',
        type=int,
        default=25000,
        help='æ¯æ®µçš„æœ€å¤§ token æ•¸é‡ (é è¨­: 25000)'
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='è©¦é‹è¡Œæ¨¡å¼ - åªåˆ†æä¸å¯«å…¥æ–‡ä»¶'
    )

    parser.add_argument(
        '--analyze-only',
        action='store_true',
        help='åƒ…åˆ†ææ–‡æª”ä¸åŸ·è¡Œåˆ†å‰²'
    )

    args = parser.parse_args()

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file_path = Path(args.file)
    if not file_path.exists():
        safe_print(f"âŒ éŒ¯èª¤: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
        return 1

    if not file_path.suffix == '.md':
        safe_print(f"âŒ éŒ¯èª¤: åªæ”¯æ´ Markdown æ–‡ä»¶ (.md)")
        return 1

    # åˆ†ææ–‡æª”
    analysis = analyze_document(file_path)

    # å¦‚æœåªæ˜¯åˆ†æï¼Œåˆ°æ­¤çµæŸ
    if args.analyze_only:
        safe_print("\nâœ… åˆ†æå®Œæˆ")
        return 0

    # å¦‚æœéœ€è¦åˆ†å‰²ï¼ŒåŸ·è¡Œåˆ†å‰²
    if analysis['needs_split']:
        perform_split(file_path, analysis, args.max_tokens, args.dry_run)
    else:
        safe_print("\nâœ… æ–‡æª”ç„¡éœ€åˆ†å‰²")

    return 0


if __name__ == '__main__':
    exit(main())
